{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 3.]\n",
      " [2. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "['Hello', 'Hi', '你好']\n"
     ]
    }
   ],
   "source": [
    "# 字典特征抽取\n",
    "# 针对数据是字典，对类别特征one-hot编码\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dic = DictVectorizer(sparse=False)\n",
    "dic_data = [{\"你好\":3},{\"Hello\":2},{\"Hi\":1}]\n",
    "data = dic.fit_transform(dic_data)\n",
    "print(data) # 类型是Sparse，只显示有数据的位置和数据\n",
    "print(dic.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 1 1 0]\n",
      " [1 1 1 0 1 1 0 1]]\n",
      "['dislike', 'is', 'lift', 'like', 'long', 'python', 'short', 'to']\n"
     ]
    }
   ],
   "source": [
    "# 文本特征抽取\n",
    "# 抽取每个单词作为特征名one-hot编码，单个字母不统计\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = [\"Lift is short,i like python\",\"Lift is to long,i dislike python\"]\n",
    "cv = CountVectorizer()\n",
    "data = cv.fit_transform(text)\n",
    "print(data.toarray())\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sklearn默认支持', '中文']\n",
      "[[0 1]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "# 默认支持中文，但是只通过空格进行分词\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "data = cv.fit_transform([\"中文\",\"sklearn默认支持 中文\"])\n",
    "print(cv.get_feature_names())\n",
    "print(data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装jieba分词，再进行抽取\n",
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12', '25', '26', '两个', '严格', '严格执行', '中共中央政治局', '中央', '中央政治局', '主题', '习近平', '以来', '会议', '作为', '作风', '做到', '党中央', '党性', '八项', '关于', '决策', '准备', '凝聚', '分析', '创新', '力量', '加强', '勇于', '十九', '发言', '召开', '同志', '四个', '围绕', '坚决', '坚定', '坦诚', '实施细则', '实际', '审议', '对照检查', '工作', '带头', '开展批评', '强化', '思想', '总书记', '情况', '意见', '意识', '批示', '报告', '担当', '指示', '按照', '撰写', '方向', '方面', '日至', '明确', '有关', '材料', '查摆', '树牢', '检查', '武装', '民主', '求真务实', '深刻', '理论', '生活', '目的', '统一', '维护', '联系', '自信', '自我', '自我批评', '若干', '落到实处', '要求', '规定', '认识', '谈心', '谈话', '负责同志', '贯彻执行', '贯彻落实', '达到', '进一步', '进行', '逐个', '部署', '重要', '随后', '集中统一', '领导', '首先']\n",
      "[[1 1 1 1 0 2 3 1 1 1 1 0 0 1 1 1 3 1 1 1 2 0 0 1 1 0 1 1 0 0 1 0 2 0 2 1\n",
      "  0 1 2 0 0 1 3 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0\n",
      "  0 2 4 1 1 1 1 1 0 2 0 0 0 0 0 2 0 0 1 0 2 1 0 1 1 0]\n",
      " [0 0 0 0 1 0 0 1 4 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 3 0 3 0 1 0 0\n",
      "  1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 2 1 2 0 0 0 0 0 1 0 0 1\n",
      "  1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 2 1 0 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "text1 = \"中共中央政治局于12月25日至26日召开民主生活会，以强化创新理论武装，树牢“四个意识”，坚定“四个自信”，坚决做到“两个维护”，勇于担当作为，以求真务实作风坚决把党中央决策部署落到实处为主题，联系中央政治局工作，联系带头严格执行《中共中央政治局关于加强和维护党中央集中统一领导的若干规定》，联系带头贯彻落实习近平总书记重要指示批示和党中央决策部署的实际，联系带头严格执行《中共中央政治局贯彻落实中央八项规定实施细则》的实际，进行自我检查、党性分析，开展批评和自我批评。\"\n",
    "text2 = \"会前，有关方面作了准备。中央政治局的同志同有关负责同志谈心谈话，围绕主题进行查摆，撰写了发言材料。会议首先审议了《关于党的十九大以来中央政治局贯彻执行中央八项规定情况的报告》。随后，中央政治局的同志逐个发言，按照要求进行对照检查。中央政治局同志的发言认识深刻，查摆严格，意见坦诚，达到了进一步统一思想、明确方向、凝聚力量的目的。\"\n",
    "text1 = \" \".join(jieba.cut(text1))\n",
    "text2 = \" \".join(jieba.cut(text2))\n",
    "cv = CountVectorizer()\n",
    "data = cv.fit_transform([text1,text2])\n",
    "print(cv.get_feature_names())\n",
    "print(data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF \n",
    "# TF:词频\n",
    "# IDF：逆文档频率 = log(总文档/该词出现的文档数)，当这个词为“因为”，“因此”值会较小\n",
    "# TF*IDF 词的重要性\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tiv = TfidfVectorizer()\n",
    "text1 = \"中共中央政治局于12月25日至26日召开民主生活会，以强化创新理论武装，树牢“四个意识”，坚定“四个自信”，坚决做到“两个维护”，勇于担当作为，以求真务实作风坚决把党中央决策部署落到实处为主题，联系中央政治局工作，联系带头严格执行《中共中央政治局关于加强和维护党中央集中统一领导的若干规定》，联系带头贯彻落实习近平总书记重要指示批示和党中央决策部署的实际，联系带头严格执行《中共中央政治局贯彻落实中央八项规定实施细则》的实际，进行自我检查、党性分析，开展批评和自我批评。\"\n",
    "text2 = \"会前，有关方面作了准备。中央政治局的同志同有关负责同志谈心谈话，围绕主题进行查摆，撰写了发言材料。会议首先审议了《关于党的十九大以来中央政治局贯彻执行中央八项规定情况的报告》。随后，中央政治局的同志逐个发言，按照要求进行对照检查。中央政治局同志的发言认识深刻，查摆严格，意见坦诚，达到了进一步统一思想、明确方向、凝聚力量的目的。\"\n",
    "text1 = \" \".join(jieba.cut(text1))\n",
    "text2 = \" \".join(jieba.cut(text2))\n",
    "data = tiv.fit_transform([text1,text2])\n",
    "print(tiv.get_feature_names())\n",
    "print(data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理，API在 sklearn.preprocessing中，包括常用的归一化，标准化，缺失值。\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 归一化（保证特征同等重要）  \n",
    "$x'=\\frac{x-min}{max-min}$  \n",
    "$x''= x'*(mx-mi)+mi$  \n",
    "mx默认为1，mi默认为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -39.08904    21.137325   53.933342]\n",
      " [-166.22987    24.277664  -24.470499]]\n",
      "[[0.9999999 0.        1.       ]\n",
      " [0.        1.        0.       ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.randn(2,3).numpy()*100\n",
    "print(data)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "data = mms.fit_transform(data)\n",
    "print(data)\n",
    "# 如果某个特征为体重，某个特征为每年开车里程数，这两个数字不在同一个数量级，若不进行处理，前者的值可以忽略不计\n",
    "# 异常点对最大值最小值影响很大，所以归一化不够robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 标准化  \n",
    "最常见的让样本特征等权重的方式，将样本map到mean为0，variance为1的区间  \n",
    "$x'=\\frac{x-\\mu }{\\sigma}$  \n",
    "$\\mu$为平均值，$\\sigma$为标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15. 14. 12. 14. 14. 13. 16. 16. 13. 19.]]\n",
      "[[15. 14. 12. 14. 14. 13. 16. 16. 13. 19.]\n",
      " [12. 15. 14. 15. 15. 11. 14. 16. 10. 17.]]\n",
      "[[ 1. -1. -1. -1. -1.  1.  1.  0.  1.  1.]\n",
      " [-1.  1.  1.  1.  1. -1. -1.  0. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "data1 = np.array([np.random.randint(low=10,high=20) for _ in range(10)]).reshape(1,10)\n",
    "data2 = np.array([np.random.randint(low=10,high=20) for _ in range(10)]).reshape(1,10)\n",
    "data1 = data1.astype(np.float32) #numpy中改变dtype不能直接修改dtype属性，只能用astype()\n",
    "data2 = data2.astype(np.float32)\n",
    "data = np.vstack([data1,data2])\n",
    "print(data)\n",
    "res = ss.fit_transform(data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 缺失值处理  \n",
    "可以使用pandas的`dropna`删除NaN或`fillna`填补NaN（如使用列均值），如果确实值不是NaN，可以使用`replace('?',np.nan)`将缺失值替换为NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "im = Imputer(missing_values=\"nan\",strategy=\"mean\",axis=0) # axis=0是列，和pandas中相同\n",
    "im.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter过滤式\n",
    "删除低方差的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3]\n",
      " [1 2]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "vts = VarianceThreshold(threshold=0.0)\n",
    "data = vts.fit_transform([[0,2,3],[0,1,2],[0,1,1]])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主成分分析\n",
    "PCA是一种损失少量信息，降低数据维度的数学方法  \n",
    "遇到推荐系统中那样稀疏的数据，可以使用PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.9) # n_components为小数时，表示保留的百分比，一般为0.9 - 9.5，即保留90%到95%的信息。为整数时表示保留维度数（不常用）\n",
    "data = pca.fit_transform(row_data) # 数据需要是二维的，每一行为一条记录，每列为一个特征，可以使用交叉表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据和合并表\n",
    "### 读取数据\n",
    "`pd.readcsv(filename)`\n",
    "### 通过相同的一列合并表\n",
    "通过`pd.merge(tab1,tab2,on=['user_id','user_id'])`来合并表\n",
    "### 交叉表（推荐系统中使用过）\n",
    "`pd.crosstab(tab[\"user_id\"],tab[\"product_id\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习算法\n",
    "`fit_transform()`是`fit()`和`transform()`的组合\n",
    "- `fit()`输入训练集数据进行训练\n",
    "- `predict(x_test)`获取预测值\n",
    "- `score(x_test,y_test)`进行评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"estimator_api.png\" width=50% >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
